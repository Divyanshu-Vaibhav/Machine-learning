# -*- coding: utf-8 -*-
"""Asgn3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c36xxUwoH5bEEvL2kOe6ye1ys0Mxjh7S
"""

#Divyanshu Vaibhav
#21BT10014
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

#Opening and cleaning the data
df=pd.read_csv('/content/drive/MyDrive/Mlfa/asgn3/Iris.csv')

#dropping ID column
df1=df.drop('Id',axis=1)

#Encoding numerical values for species(Classes)
label_encoder=LabelEncoder()
df1['Species'] = label_encoder.fit_transform(df1['Species'])

#Using Scaler to scale down all data
scaler=StandardScaler()
#Taking feature columns except last species column
feature_col=df1.columns[:-1]
df1[feature_col] = scaler.fit_transform(df1[feature_col])
print(df1)

#Implementing LOG_MUL_GRAD from scatch
class LogisticRegressionMultiClass:
    def __init__(self,no_classes,learning_rate,no_epochs,batch_size):
        self.no_classes=no_classes
        self.learning_rate=learning_rate
        self.no_epochs=no_epochs
        self.batch_size=batch_size
        self.class_probabilities=[[] for _ in range(self.no_classes)]

    def soft_max(self,z):
        #z will be a matrix with no of sample data*no of classes
        exp_z=np.exp(z-np.max(z,axis=1,keepdims=True))
        # After taking exp of logit and dividing with sum of exp of all data in a class
        return exp_z/np.sum(exp_z,axis=1,keepdims=True)

    def initialize_weights(self,no_features):
        self.W=np.zeros((no_features,self.no_classes))
        self.b=np.zeros(self.no_classes)

    def one_hot_encode(self,y):
        one_hot=np.zeros((len(y),self.no_classes))
        for i in range(len(y)):
            one_hot[i, y[i]] = 1
        return one_hot

    def fit(self, X, y):
        no_samples,no_features=X.shape
        self.initialize_weights(no_features)

        for epoch in range(self.no_epochs):
            # shuffling data as our data has data belonging to one class continous row wise
            permutation = np.random.permutation(no_samples)
            X_shuffled=X[permutation]
            y_shuffled=y[permutation]

            for i in range(0, no_samples, self.batch_size):
                X_batch=X_shuffled[i:i+self.batch_size]
                y_batch=y_shuffled[i:i+self.batch_size]

                #Computing logits
                # X = no of sample data*no of features
                # W = no of features*no of classes
                # X*W will give logit matrix with no of sample data*no of classes
                logits = np.dot(X_batch,self.W)+self.b
                #soft max gives a matrix of shape equal to no of sample data * no of classes
                # basically each data row of X will have three probabilites corresponding to all three classes
                probabilities=self.soft_max(logits)

                #Calculating gradients
                #One hot encode is matrix of shape no of sample data * no of classes with only cell in a row equal to 1
                #of which the sample data point actually belong to
                #Grad is Average of (X*Prob-One hot code)
                #Prob of Xi,yi data point of X will have Prob row as [a,b,c] and One hot code as [0,1,0] if yi corresponds to class 1
                grad_W=(1/self.batch_size)*np.dot(X_batch.T,(probabilities-self.one_hot_encode(y_batch)))
                grad_b=(1/self.batch_size)*np.sum(probabilities-self.one_hot_encode(y_batch),axis=0)

                #Updating weights and biases
                self.W-=self.learning_rate*grad_W
                self.b-=self.learning_rate*grad_b

            mean_probabilities=np.mean(probabilities,axis=0)
            for class_idx in range(self.no_classes):
                self.class_probabilities[class_idx].append(mean_probabilities[class_idx])

    def plot_class_probabilities(self):
        epochs=range(1,self.no_epochs+1)
        print(np.array(self.class_probabilities).shape)
        plt.plot(epochs,self.class_probabilities[0],label=f'Class {0}',marker='^')
        plt.plot(epochs,self.class_probabilities[1],label=f'Class {1}',marker='o')
        plt.plot(epochs,self.class_probabilities[2],label=f'Class {2}')
        plt.xlabel('Epochs')
        plt.ylabel('Mean Class Probability')
        plt.legend()
        plt.title('Mean Class Probabilities vs Epochs for each Class')
        plt.show()

    def predict(self,X):
        logits=np.dot(X,self.W)+self.b
        probabilities=self.soft_max(logits)
        return np.argmax(probabilities,axis=1)

# making X and y numpy arrays of input and output
X=df1.values
y=df1['Species'].values

# ---------------------------------- EXPERIMENT 1 ------------------------------------


#Breaking the dataset into train,validation,test data in ratio 60:20:20
X_train,X_temp,y_train,y_temp=train_test_split(X,y,test_size=0.4,random_state=42)
X_val,X_test,y_val,y_test=train_test_split(X_temp,y_temp,test_size=0.5,random_state=42)

#Learning rates to be checked as given in the assignment
learning_rates=[1e-5,1e-4,1e-3,1e-2,0.05,0.1]

#Initialized accuracies list to store accuracy of each learning rate
accuracies=[]

#Changing the learning rate
for learning_rate in learning_rates:
    model=LogisticRegressionMultiClass(no_classes=3,learning_rate=learning_rate,no_epochs=50,batch_size=30)
    model.fit(X_train,y_train)

    #Making predictions on the validation data
    y_pred=model.predict(X_val)

    #getting accuracy and storing in the list accuracies
    accuracy=accuracy_score(y_val,y_pred)
    accuracies.append(round(accuracy,3))

# Plotting Percentage Accuracy vs Learning Rate
plt.figure(figsize=(10,6))
plt.semilogx(learning_rates,accuracies,marker='^')
plt.xlabel('Learning rate')
plt.ylabel('Accuracy on Validation Data')
plt.title('Accuracy v/s Validation data (Experiment 1)')
plt.grid(True)
plt.show()

# Finding the best learning rate based on the highest accuracy
best_learning_rate = 0.1
#best_learning_rate = learning rate for which accuracy is maximum
print("The accuracy list is:",(accuracies))
print(f"Best learning rate is {best_learning_rate},and corresponding best accuracy is {max(accuracies)}.")

# ---------------------------------- EXPERIMENT 2 -----------------------------------
class_0x = []
class_1x = []
class_2x = []
class_0y = []
class_1y = []
class_2y = []
# iterating X_train and y_train to divide data into list of there classes
for i in range(len(X_train)):
    if y_train[i] == 0:
        class_0x.append(X_train[i])
        class_0y.append(y_train[i])
    elif y_train[i] == 1:
        class_1x.append(X_train[i])
        class_1y.append(y_train[i])
    elif y_train[i] == 2:
        class_2x.append(X_train[i])
        class_2y.append(y_train[i])

# Converting to np array so that our logistic regression class can use it
class_0x = np.array(class_0x)
class_1x = np.array(class_1x)
class_2x = np.array(class_2x)

class_0y = np.array(class_0y)
class_1y = np.array(class_1y)
class_2y = np.array(class_2y)
# Applying for all sets one by one

#For class 0
model=LogisticRegressionMultiClass(3,best_learning_rate,50,30)
# Here we train with data points purely belonging to class 0
# to show how the Posterior Probability for that class(Class 0) increases after
# updating weights and bias after every epoch and converges to 1
model.fit(class_0x,class_0y)
model.plot_class_probabilities()
print("Probabilities per epoch set for dataset for class 0:")
print(model.class_probabilities[0])
print(model.class_probabilities[1])
print(model.class_probabilities[2])

#For  class 1
model=LogisticRegressionMultiClass(3,best_learning_rate,50,30)
model.fit(class_1x,class_1y)
model.plot_class_probabilities()
print("Probabilities per epoch for dataset for class 1:")
print(model.class_probabilities[0])
print(model.class_probabilities[1])
print(model.class_probabilities[2])

#For class 2

model=LogisticRegressionMultiClass(3,best_learning_rate,50,30)
model.fit(class_1x,class_2y)
model.plot_class_probabilities()
print("Probabilities per epoch for dataset for class 2:")
print(model.class_probabilities[0])
print(model.class_probabilities[1])
print(model.class_probabilities[2])

# --------------------------------  EXPERIMENT 3 -----------------------------------

model=LogisticRegressionMultiClass(3,best_learning_rate,50,30)
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
class_labels=['0','1','2']

#Confusion matrix
conf_matrix=confusion_matrix(y_test,y_pred)
print("Confusion Matrix:")
print(conf_matrix)

#Plotting the heatmap for confusion matrix
plt.figure(figsize=(10,8))
sns.heatmap(conf_matrix,annot=True,fmt="d",cmap="Reds",xticklabels=class_labels,yticklabels=class_labels)
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('Confusion Matrix')
plt.show()

#Classification report containing all performance metrics precision,recall,f1-score and accuracy of the model
class_rep=classification_report(y_test,y_pred,target_names=class_labels)
print("Classification report are as follows:")
print(class_rep)