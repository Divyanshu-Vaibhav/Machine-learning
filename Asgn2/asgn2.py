#Divyanshu Vaibhav
#21BT10014
# -*- coding: utf-8 -*-
"""Asgn2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QYgDSG16YbvHcQhS0wiiOUE5dCkr8YRp
"""


import numpy as np
import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
# load the dataset
df = pd.read_csv('/content/drive/MyDrive/Mlfa/asgn2/dataset.csv')

df

df.describe(include='all').T

df['Gender Code'] = df['Gender'].replace({'M': 0, 'F': 1})

for column in df.columns:
  nan_count = df[column].isna().sum()

  print("Percentage Nan value:",nan_count/(df[column].count()+nan_count)*100," Total count:",df[column].count()+nan_count,"Nan count:",nan_count,"feature name:",column)

"""
# Dropped Product category 3 because it has >50 percent Nan value"""

df

"""# Experiment 1: Exploratory Data Analysis"""

import seaborn as sns
import matplotlib.pyplot as plt
import plotly
import plotly.express as px
# Visualize the distribution of 'Age' using a histogram
fig=px.histogram(df,x='Age',color='Gender',title='Age Distribution')
fig.show()

# Visualize the distribution of 'Purchase' using a box plot
# plt.figure(figsize=(8, 6))
# sns.boxplot(data=df, y='Purchase')
# plt.title('Distribution of Purchase')
# plt.show()

fig=px.histogram(df,x='Age',y='Purchase',color='Gender',title='Purchase amt Vs Age')
fig.show()

"""# Age 26-35 is the Biggest spender"""

fig=px.histogram(df,x='Occupation',y='Purchase',nbins=21,color='Gender',title='Purchasing power of different occupations')
fig.update_layout(bargap=0.1)
fig.show()
fig=px.scatter(df,x='Age',y='Occupation',color='Gender',title='Age group of Occupations')

fig.show()

"""#Occupation 4 has the highest purchasing power. The company should target them.

"""

temp=df.drop(columns=['User_ID','Product_ID'])
plt.figure(figsize=(20, 20))
sns.heatmap(temp.corr(), cmap='Blues', annot=True)
plt.title('Correlation Matrix');

print(df.describe())
mode_prod_cat2=df['Product_Category_2'].mode().iloc[0]
print(mode_prod_cat2)
df['Product_Category_2'].fillna(mode_prod_cat2, inplace=True)
df

col_to_drop=['Gender','User_ID','Product_ID']
df1=df.drop(columns=col_to_drop,axis=1)
df1

df1['City_Category']=df1['City_Category'].replace({'A':0,'B':1,'C':2})
df1['Stay_In_Current_City_Years']=df1['Stay_In_Current_City_Years'].replace({'4+':4})
print(df['Age'].dtype)
print(df['Age'].unique)
df1['Age']=df1['Age'].replace({'0-17':0,'18-25':1,'26-35':2,'36-45':3,'46-50':4,'51-55':5,'55+':6})
df1

df2=df1.drop('Purchase',axis=1)
df2=df2.drop('Product_Category_3',axis=1)
#Extracting out value from dataframes into 2D numpy array
X=df2.values

df3=df1['Purchase']
y=df3.values
colofones=np.ones((X.shape[0],1))
#Adding column of ones to add bias coefficient
X=np.hstack((X,colofones))
#Converting all to int
X=X.astype(int)
print(y)
print(X)

"""#Experiment 2: Implementing Lin Model Closed"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

#Splitting our data to 80:20 train:test split
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=10)

# w= A^-1*b  where A=sum of all X.T*X and b=X.T*y
#Without Scaled Data

W_weights1 = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)
# print(W_weights1)

y_pred = X_test.dot(W_weights1)
# print(y_pred)
# Calculating Mean Squared Error (MSE)
def mean_squared_error(y_true, y_pred):
    return np.mean((np.square(y_true - y_pred))/len(y_test))
# Calculated MSE
mse = mean_squared_error(y_test, y_pred)

print("MSE value without scaled data:",mse)


#With Scaled Data
X1=df2.values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X1)
colofones=np.ones((X_scaled.shape[0],1))
#Adding column of ones to add bias coefficient
X_scaled=np.hstack((X_scaled,colofones))
#Converting all to int
X_scaled=X_scaled.astype(int)
# Split the scaled data into train and test sets
X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
print(X_test_scaled)
W_weights2 = np.linalg.inv(X_train_scaled.T.dot(X_train_scaled)).dot(X_train_scaled.T).dot(y_train)
print(W_weights2)
y_pred = X_test_scaled.dot(W_weights2)
mse = mean_squared_error(y_test, y_pred)

print("MSE value with scaled data:",mse)

"""#Experiment 3: Implementing Lin Model Grad"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.linear_model import SGDRegressor

import matplotlib.pyplot as plt

X=X_scaled
#Running on scaled data
# Splitting data into train,validate,test in 60:20:20 ratio
X_train, X_temp, y_train, y_temp = train_test_split(X,y, test_size=0.4, random_state=10)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=10)
def mean_squared_error(y_true, y_pred):
    return np.mean((np.square(y_true-y_pred))/len(y_true))
# Defined minibatch size and number of epochs
batch_size = 256
epochs = 50
learn_rate = [1e-5, 1e-4, 1e-3, 1e-2, 0.05, 0.1]  # Example learning rates

mse_scores = []


# Iterating over different learning rates
for lr in learn_rate:
    # Initialising the SGDRegressor with gradient descent
    model = SGDRegressor(learning_rate='constant', eta0=lr, max_iter=1, random_state=0, warm_start=True)
    # Mini-batch training loop
    for _ in range(epochs):
        for i in range(0, len(X_train_scaled), batch_size):
            X_batch=X_train[i:i + batch_size]
            y_batch=y_train[i:i + batch_size]
            model.partial_fit(X_batch,y_batch)

    remaining_data_start = (len(X_train)//batch_size) * batch_size
    if remaining_data_start < len(X_train):
        X_batch=X_train[remaining_data_start:]
        y_batch=y_train[remaining_data_start:]
        model.partial_fit(X_batch, y_batch)

    # Prediction on validation data
    y_pred = model.predict(X_val)

    # Calculating mean squared error
    mse = mean_squared_error(y_val, y_pred)
    mse_scores.append(mse)

# Plotting the results
plt.figure(figsize=(10, 6))
plt.plot(learn_rate, mse_scores, marker='o')
plt.xscale('log')
plt.xlabel('Learning Rate')
plt.ylabel('Mean Squared Error')
plt.title('Effect of Learning Rate on Validation Data (Minibatch)')
plt.grid(True)
plt.show()

# Finding the best learning rate
best_lr_idx = np.argmin(mse_scores)
best_lr = learn_rate[best_lr_idx]
print(f"Best learning rate: {best_lr:.6f}")




# Experiment 4: Ridge Regression with Minibatch Gradient Descent
mse_epochR=[]
alpha_values=np.arange(0.0, 1.1, 0.1)  # Example alpha values
mse_values_ridge=[]
num_epochs=50
minibatch_size=256
for alpha in alpha_values:
    model_ridge = Ridge(alpha=alpha)

    for epoch in range(num_epochs):
        for i in range(0, len(X_train), minibatch_size):
            X_batch = X_train[i:i + minibatch_size]
            y_batch = y_train[i:i + minibatch_size]
            model_ridge.fit(X_batch, y_batch)

        y_pred = model_ridge.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        mse_epochR.append(mse)
    mse_values_ridge.append(sum(mse_epochR)/len(mse_epochR))
    mse_epochR=[]


plt.plot(alpha_values, mse_values_ridge,label='Ridge Regression')
plt.xlabel('Alpha')
plt.ylabel('Mean Squared Error')
plt.title('MSE vs Alpha (Ridge Hyperparameter)')
plt.grid(True)
plt.legend()
plt.show()


#The best alpha we can observe from the Graph
best_alpha=0.6






#Experiment 5
# Testing performance of all three models through MSE on 20 percent test data

model=SGDRegressor(learning_rate='constant',eta0=best_lr,max_iter=50,random_state=0, warm_start=True)
model.fit(X_train_scaled,y_train)
y_pred=model.predict(X_val)

# Calculate mean squared error
mse_reg_best=mean_squared_error(y_test, y_pred)
# print(mse_reg_best)


ridge_model=Ridge(alpha=best_alpha, solver='sag', max_iter=50, random_state=42)
ridge_model.fit(X_train_scaled, y_train)

y_pred_val=ridge_model.predict(X_val)
mse_ridge=mean_squared_error(y_val, y_pred_val)
# print(mse_ridge)