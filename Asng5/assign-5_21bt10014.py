#Divyanshu Vaibhav
#21BT10014



# -*- coding: utf-8 -*-
"""Asgn_forward_nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NRWDkMolSJiClyis-I2V9f-WFoTKEHIz
"""




#-------------------------------------------------------------------CASE1-----------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
from torch.utils.data import DataLoader

#Loading image dataset from mnist

train_dataset=dsets.MNIST(root='./data',
                            train=True,
                            transform=transforms.ToTensor(),
                            download=True)

test_dataset=dsets.MNIST(root='./data',
                           train=False,
                           transform=transforms.ToTensor())



batch_size=100
n_itrs=4500
num_epochs=n_itrs/(len(train_dataset)/batch_size)
num_epochs=int(num_epochs)

train_loader=DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader=DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)

#Creating a class for our nn model


#We need to create 3 models as we are given different no of hidden layers for them

#This dimension is common to all out three models that we will make

input_dim=28*28
hidden_dim=50
output_dim=10

#Class 1 feedforward nn model with 2 hidden layers

class FeedforNNModel1(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(FeedforNNModel1, self).__init__()
        # Linear function
        self.fc1 = nn.Linear(input_dim,hidden_dim)
        # Non-linearity function
        self.sigmoid1 = nn.Sigmoid()
        self.fc2 = nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid2=nn.Sigmoid()
        self.fc3=nn.Linear(hidden_dim,output_dim)
        #Initialising using kaiming normal distribution
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.kaiming_normal_(self.fc3.weight)


    def forward(self,x):
        # Linear function  # LINEAR
        out = self.fc1(x)
        # Non-linearity  # NON-LINEAR
        out = self.sigmoid1(out)
        # Linear function # LINEAR
        out = self.fc2(out)
        out=self.sigmoid2(out)
        # Output (linear fuction)
        out=self.fc3(out)
        return out

model = FeedforNNModel1(input_dim,hidden_dim,output_dim)

#Using Cross Entropy as our loss criteria
criterion=nn.CrossEntropyLoss()

learning_rate=0.1

#Using mini batch gradient descent with momentum, it will help in accerlerating convergence
optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)

#Training our model and finding best accuracy
acc=[]
l = [] # for creating a list of losses
it = [] # for creating a list of iterations
itr = 0
best_acc_model1=0
for epoch in range(num_epochs):
    for i, (images,labels) in enumerate(train_loader):
        # Loading images with gradient accumulation capabilities
        images=images.view(-1,28*28).requires_grad_()
        labels=labels
         # Cleared gradients w.r.t. parameters
        optimizer.zero_grad()

        #Forward pass to get output/logits
        outputs=model(images)

        #Calculating cross entropy loss
        loss=criterion(outputs,labels)

        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()

        itr += 1

        if itr % 100==0:
            # Calculate Accuracy
            correct = 0
            total = 0
            # Iterate through test dataset
            for images,labels in test_loader:
                #Load images with gradient accumulation capabilities
                images=images.view(-1,28*28).requires_grad_()

                #Forward pass only to get output
                outputs=model(images)

                # Get predictions from the maximum value
                _, predicted = torch.max(outputs.data, 1)

                #Total number of labels
                total+=labels.size(0)
                #Total correct predictions
                correct+=(predicted==labels).sum()

            accuracy=100*correct/total
            best_acc_model1=max(accuracy,best_acc_model1)


            it.append(itr)
            l.append(loss.item())
            acc.append(accuracy)


print('The best accuracy for neural net with 2 hidden layers is {}'.format(best_acc_model1))
plt.plot(it,acc)
plt.ylabel('Accuracy for 1st nn')
plt.xlabel('Epoch')

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
from torch.utils.data import DataLoader

#Loading image dataset from mnist

train_dataset=dsets.MNIST(root='./data',
                            train=True,
                            transform=transforms.ToTensor(),
                            download=True)

test_dataset=dsets.MNIST(root='./data',
                           train=False,
                           transform=transforms.ToTensor())



batch_size=100
n_itrs=4200
num_epochs=n_itrs/(len(train_dataset)/batch_size)
num_epochs=int(num_epochs)

train_loader=DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader=DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)

input_dim=28*28
hidden_dim=50
output_dim=10
#Class 2 feedforward nn model with 4 hidden layers

class FeedforNNModel2(nn.Module):
    def __init__(self,input_dim,hidden_dim,output_dim):
        super(FeedforNNModel2, self).__init__()
        # Linear function
        self.fc1 = nn.Linear(input_dim,hidden_dim)
        # Non-linearity function
        self.sigmoid1 = nn.Sigmoid()
        self.fc2=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid2=nn.Sigmoid()
        self.fc3=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid3=nn.Sigmoid()
        self.fc4=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid4=nn.Sigmoid()
        self.fc5=nn.Linear(hidden_dim,output_dim)
        #Initialising using kaiming normal distribution
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.kaiming_normal_(self.fc3.weight)
        nn.init.kaiming_normal_(self.fc4.weight)
        nn.init.kaiming_normal_(self.fc5.weight)


    def forward(self,x):
        # Linear function  # LINEAR
        out=self.fc1(x)
        out=self.sigmoid1(out)
        out=self.fc2(out)
        out=self.sigmoid2(out)
        out=self.fc3(out)
        out=self.sigmoid3(out)
        out=self.fc4(out)
        out=self.sigmoid4(out)
        out=self.fc5(out)

        return out

model=FeedforNNModel2(input_dim,hidden_dim,output_dim)

#Using Cross Entropy as our loss criteria
criterion=nn.CrossEntropyLoss()

learning_rate=0.01

#Using mini batch gradient descent with momentum, it will help in accerlerating convergence
optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)

#Training our model and finding best accuracy

acc= [] # for creating a list of accuracies over the iterations
it = [] # for creating a list of iterations
l=[]
itr = 0
best_acc_model2=0
for epoch in range(num_epochs):
    for i, (images,labels) in enumerate(train_loader):
        # Loading images with gradient accumulation capabilities
        images=images.view(-1,28*28).requires_grad_()
        labels=labels
         # Cleared gradients w.r.t. parameters
        optimizer.zero_grad()

        #Forward pass to get output/logits
        outputs=model(images)

        #Calculating cross entropy loss
        loss=criterion(outputs,labels)

        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()

        itr+=1

        if itr % 100== 0:
            # Calculate Accuracy
            correct = 0
            total = 0
            # Iterate through test dataset
            for images,labels in test_loader:
                # Load images with gradient accumulation capabilities
                images=images.view(-1,28*28).requires_grad_()

                # Forward pass only to get logits/output
                outputs=model(images)

                # Get predictions from the maximum value
                _, predicted = torch.max(outputs.data, 1)

                # Total number of labels
                total+=labels.size(0)
                # Total correct predictions
                correct+=(predicted==labels).sum()

            accuracy = 100*correct/total
            best_acc_model2=max(accuracy,best_acc_model2)


            it.append(itr)
            l.append(loss.item())
            acc.append(accuracy)


print('The best accuracy for neural net with 4 hidden layers {}'.format(best_acc_model2))
plt.plot(it,acc)
plt.ylabel('Accuracy for 2nd nn')
plt.xlabel('Epoch')

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
from torch.utils.data import DataLoader

#Loading image dataset from mnist

train_dataset=dsets.MNIST(root='./data',
                            train=True,
                            transform=transforms.ToTensor(),
                            download=True)

test_dataset=dsets.MNIST(root='./data',
                           train=False,
                           transform=transforms.ToTensor())



batch_size=100
n_itrs=4200
num_epochs=n_itrs/(len(train_dataset)/batch_size)
num_epochs=int(num_epochs)

train_loader=DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader=DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)

input_dim=28*28
hidden_dim=50
output_dim=10
#Class 3 feedforward nn model with 8 hidden layers

class FeedforNNModel3(nn.Module):
    def __init__(self,input_dim,hidden_dim,output_dim):
        super(FeedforNNModel3, self).__init__()
        # Linear function
        self.fc1 = nn.Linear(input_dim,hidden_dim)
        # Non-linearity function
        self.sigmoid1 = nn.Sigmoid()
        self.fc2=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid2=nn.Sigmoid()
        self.fc3=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid3=nn.Sigmoid()
        self.fc4=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid4=nn.Sigmoid()
        self.fc5=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid5=nn.Sigmoid()
        self.fc6=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid6=nn.Sigmoid()
        self.fc7=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid7=nn.Sigmoid()
        self.fc8=nn.Linear(hidden_dim,hidden_dim)
        self.sigmoid8=nn.Sigmoid()
        self.fc9=nn.Linear(hidden_dim,output_dim)
        #Initialising using kaiming normal distribution
        nn.init.kaiming_normal_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.kaiming_normal_(self.fc3.weight)
        nn.init.kaiming_normal_(self.fc4.weight)
        nn.init.kaiming_normal_(self.fc5.weight)
        nn.init.kaiming_normal_(self.fc6.weight)
        nn.init.kaiming_normal_(self.fc7.weight)
        nn.init.kaiming_normal_(self.fc8.weight)
        nn.init.kaiming_normal_(self.fc9.weight)


    def forward(self,x):
        # Linear function Â # LINEAR
        out=self.fc1(x)
        out=self.sigmoid1(out)
        out=self.fc2(out)
        out=self.sigmoid2(out)
        out=self.fc3(out)
        out=self.sigmoid3(out)
        out=self.fc4(out)
        out=self.sigmoid4(out)
        out=self.fc5(out)
        out=self.sigmoid5(out)
        out=self.fc6(out)
        out=self.sigmoid6(out)
        out=self.fc7(out)
        out=self.sigmoid7(out)
        out=self.fc8(out)
        out=self.sigmoid8(out)
        out=self.fc9(out)

        return out


model=FeedforNNModel3(input_dim,hidden_dim,output_dim)

#Using Cross Entropy as our loss criteria
criterion=nn.CrossEntropyLoss()

learning_rate=0.01

#Using mini batch gradient descent with momentum, it will help in accerlerating convergence
optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9)

#Training our model and finding best accuracy

acc= [] # for creating a list of accuracies over the iterations
it = [] # for creating a list of iterations
l=[]
itr = 0
best_acc_model3=0
for epoch in range(num_epochs):
    for i, (images,labels) in enumerate(train_loader):
        # Loading images with gradient accumulation capabilities
        images=images.view(-1,28*28).requires_grad_()
        labels=labels
         # Cleared gradients w.r.t. parameters
        optimizer.zero_grad()

        #Forward pass to get output/logits
        outputs=model(images)

        #Calculating cross entropy loss
        loss=criterion(outputs,labels)

        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()

        itr+=1

        if itr % 100== 0:
            # Calculate Accuracy
            correct = 0
            total = 0
            # Iterate through test dataset
            for images,labels in test_loader:
                # Load images with gradient accumulation capabilities
                images=images.view(-1,28*28).requires_grad_()

                # Forward pass only to get logits/output
                outputs=model(images)

                # Get predictions from the maximum value
                _, predicted = torch.max(outputs.data, 1)

                # Total number of labels
                total+=labels.size(0)
                # Total correct predictions
                correct+=(predicted==labels).sum()

            accuracy = 100*correct/total
            best_acc_model3=max(accuracy,best_acc_model3)


            it.append(itr)
            l.append(loss.item())
            acc.append(accuracy)


print('The best accuracy for neural net with 8 hidden layers is {}'.format(best_acc_model3))
plt.plot(it,acc)
plt.ylabel('Accuracy for 3rd nn')
plt.xlabel('Epoch')

# Now we want to perturb the networks


#-------------------------------------------------------------------CASE 2--------------------------------------------------------------------------------------

# Selecting NN 1

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
from torch.utils.data import DataLoader

# Loading the trained neural network
model = FeedforNNModel1(input_dim,hidden_dim,output_dim)

# Gettig the parameters of the hidden layers to be perturbed
params1 = model.fc1.weight
params2 = model.fc2.weight
params3 =model.fc3.weight
# Create a list to store the perturbed models
perturbed_models = []

# Perturbedd the parameters of the first hidden layer and created a new model
with torch.no_grad():
  noise = torch.randn_like(params1)*0.01
  params1 += noise.clone()
  new_model = FeedforNNModel1(input_dim,hidden_dim,output_dim)
  new_model.load_state_dict(model.state_dict())
  new_model.fc1.weight = params1
  perturbed_models.append(new_model)



# Perturbed the parameters of the second hidden layer and created a new model
with torch.no_grad():
  noise = torch.randn_like(params2) * 0.01
  params2 += noise.clone()
  new_model = FeedforNNModel1(input_dim,hidden_dim,output_dim)
  new_model.load_state_dict(model.state_dict())
  new_model.fc2.weight = params2
  perturbed_models.append(new_model)

#Perturbing the output layer
with torch.no_grad():
  noise = torch.randn_like(params3) * 0.01
  params3 += noise.clone()
  new_model = FeedforNNModel1(input_dim,hidden_dim,output_dim)
  new_model.load_state_dict(model.state_dict())
  new_model.fc3.weight = params3
  perturbed_models.append(new_model)


# Evaluate the unperturbed and perturbed models on the MNIST test data

test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

# Evaluated the unperturbed model on the test data
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images=images.view(-1,28*28).requires_grad_()
        outputs=model(images)
        _, predicted = torch.max(outputs.data, 1)
        total+=labels.size(0)
        correct+=(predicted == labels).sum()

accuracy_unperturbed = 100*correct/total

# Evaluated the perturbed models on the test data
accuracy_perturbed = []
for model in perturbed_models:
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images=images.view(-1,28*28).requires_grad_()
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum()

    accuracy = 100 * correct / total
    accuracy_perturbed.append(accuracy)

# Calculated the deviation of performance compared to the unperturbed trained network
deviation_unperturbed = 0
deviation_perturbed = []
for accuracy in accuracy_perturbed:
    deviation = accuracy - accuracy_unperturbed
    deviation_perturbed.append(deviation)

# Ranked the layers based on their performance deviation in descending order
layer_ranks = np.argsort(deviation_perturbed)[::-1]

# Printing the rankings

print('Layer rankings for 1st nn with 0 being the input layer and 2 being the output layer:', layer_ranks)

#Selecting neural network 2

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
from torch.utils.data import DataLoader

# Loading the trained neural network
model = FeedforNNModel2(input_dim,hidden_dim,output_dim)

# Getting the parameters of the hidden layers and output layer to be perturbed
params1 = model.fc1.weight
params2 = model.fc2.weight
params3 =model.fc3.weight
params4 =model.fc4.weight
params5 =model.fc5.weight

# Create a list to store the perturbed models
perturbed_models = []

# Perturbed the parameters of the first hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params1)*0.01
    params1 += noise.clone()
    new_model = FeedforNNModel2(input_dim,hidden_dim,output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc1.weight = params1
    perturbed_models.append(new_model)


# Perturbed the parameters of the second hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params2) * 0.01
    params2 += noise.clone()
    new_model = FeedforNNModel2(input_dim,hidden_dim,output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc2.weight = params2
    perturbed_models.append(new_model)

# Perturbed the parameters of the third hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params3) * 0.01
    params3 += noise.clone()
    new_model = FeedforNNModel2(input_dim,hidden_dim,output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc3.weight = params3
    perturbed_models.append(new_model)

# Perturbed the parameters of the fourth hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params4) * 0.01
    params4 += noise.clone()
    new_model = FeedforNNModel2(input_dim,hidden_dim,output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc4.weight = params4
    perturbed_models.append(new_model)

# Perturbed the parameters of the output layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params5) * 0.01
    params5 += noise.clone()
    new_model = FeedforNNModel2(input_dim,hidden_dim,output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc5.weight = params5
    perturbed_models.append(new_model)


# Evaluate the unperturbed and perturbed models on the MNIST test data

test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

# Evaluated the unperturbed model on the test data
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images=images.view(-1,28*28).requires_grad_()
        outputs=model(images)
        _, predicted = torch.max(outputs.data, 1)
        total+=labels.size(0)
        correct+=(predicted == labels).sum()

accuracy_unperturbed = 100*correct/total

# Evaluated the perturbed models on the test data
accuracy_perturbed = []
for model in perturbed_models:
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images=images.view(-1,28*28).requires_grad_()
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum()

    accuracy = 100 * correct / total
    accuracy_perturbed.append(accuracy)

# Calculated the deviation of performance compared to the unperturbed trained network
deviation_unperturbed = 0
deviation_perturbed = []
for accuracy in accuracy_perturbed:
    deviation = accuracy - accuracy_unperturbed
    deviation_perturbed.append(deviation)

# Ranked the layers based on their performance deviation in descending order
layer_ranks = np.argsort(deviation_perturbed)[::-1]

# Printing the rankings
print('Layer rankings for 2nd nn with 0 being the input layer and 4 being the output layer:', layer_ranks)

#Selecting Neural Network 3
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
from torch.utils.data import DataLoader

# Loading the trained neural network
model = FeedforNNModel3(input_dim, hidden_dim, output_dim)

# Getting the parameters of the hidden layers and output layer to be perturbed
params1 = model.fc1.weight
params2 = model.fc2.weight
params3 = model.fc3.weight
params4 = model.fc4.weight
params5 = model.fc5.weight
params6 = model.fc6.weight
params7 = model.fc7.weight
params8 = model.fc8.weight
params9 = model.fc9.weight

# Create a list to store the perturbed models
perturbed_models = []

# Perturbed the parameters of the first hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params1) * 0.01
    params1 += noise.clone()
    new_model = FeedforNNModel3(input_dim, hidden_dim, output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc1.weight = params1
    perturbed_models.append(new_model)

# Perturbed the parameters of the second hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params2) * 0.01
    params2 += noise.clone()
    new_model = FeedforNNModel3(input_dim, hidden_dim, output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc2.weight = params2
    perturbed_models.append(new_model)

# Perturbed the parameters of the third hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params3) * 0.01
    params3 += noise.clone()
    new_model = FeedforNNModel3(input_dim, hidden_dim, output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc3.weight = params3
    perturbed_models.append(new_model)

# Perturbed the parameters of the fourth hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params4) * 0.01
    params4 += noise.clone()
    new_model = FeedforNNModel3(input_dim, hidden_dim, output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc4.weight = params4
    perturbed_models.append(new_model)

# Perturbed the parameters of the fifth hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params5) * 0.01
    params5 += noise.clone()
    new_model = FeedforNNModel3(input_dim, hidden_dim, output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc5.weight = params5
    perturbed_models.append(new_model)

# Perturbed the parameters of the sixth hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params6) * 0.01
    params6 += noise.clone()
    new_model = FeedforNNModel3(input_dim, hidden_dim, output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc6.weight = params6
    perturbed_models.append(new_model)

# Perturbed the parameters of the seventh hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params7) * 0.01
    params7 += noise.clone()
    new_model = FeedforNNModel3(input_dim, hidden_dim, output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc7.weight = params7
    perturbed_models.append(new_model)

# Perturbed the parameters of the eighth hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params8) * 0.01
    params8 += noise.clone()
    new_model = FeedforNNModel3(input_dim, hidden_dim, output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc8.weight = params8
    perturbed_models.append(new_model)

# Perturbed the parameters of the ninth hidden layer and created a new model
with torch.no_grad():
    noise = torch.randn_like(params9) * 0.01
    params9 += noise.clone()
    new_model = FeedforNNModel3(input_dim, hidden_dim, output_dim)
    new_model.load_state_dict(model.state_dict())
    new_model.fc9.weight = params9
    perturbed_models.append(new_model)

# Evaluate the unperturbed and perturbed models on the MNIST test data

test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

# Evaluated the unperturbed model on the test data
correct = 0
total = 0
with torch.no_grad():
    for images, labels in test_loader:
        images = images.view(-1, 28 * 28).requires_grad_()
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum()

accuracy_unperturbed = 100 * correct / total

# Evaluated the perturbed models on the test data
accuracy_perturbed = []
for model in perturbed_models:
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.view(-1, 28 * 28).requires_grad_()
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum()

    accuracy = 100 * correct / total
    accuracy_perturbed.append(accuracy)

# Calculated the deviation of performance compared to the unperturbed trained network
deviation_unperturbed = 0
deviation_perturbed = []
for accuracy in accuracy_perturbed:
    deviation = accuracy - accuracy_unperturbed
    deviation_perturbed.append(deviation)

# Ranked the layers based on their performance deviation in descending order
layer_ranks = np.argsort(deviation_perturbed)[::-1]

# Printing the rankings
print('Layer rankings for 3rd nn with 0 being the input layer and 8 being the output layer:', layer_ranks)