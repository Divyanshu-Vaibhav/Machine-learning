# -*- coding: utf-8 -*-
#Divyanshu Vaibhav
#21BT10014


"""Asgn6_21BT10014.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12VeViX4S6_EvHwRpuQ5GUrSHcm4BQI-M
"""



#Implementing CNN Vanilla model

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

#Implementing Vanilla CNN having the same architecture as our required Resnet CNN
#But without the residual connections

class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super(CNN, self).__init__()

        # Convolutional layers
        self.l1_conv = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1)
        self.l1_relu = nn.ReLU()
        self.l1_maxpool = nn.MaxPool2d(kernel_size=2)

        self.l2_conv = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)
        self.l2_relu = nn.ReLU()
        self.l2_maxpool = nn.MaxPool2d(kernel_size=2)

        self.l3_conv = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)
        self.l3_relu = nn.ReLU()
        self.l3_maxpool = nn.MaxPool2d(kernel_size=2)

        # Fully connected layers
        self.fc_flat = nn.Flatten()
        self.fc_lin1 = nn.Linear(16 * 4 * 4, 512)
        self.fc_relu = nn.ReLU()
        self.fc_lin2 = nn.Linear(512, num_classes)

    def forward(self, x):
        # Convolutional layers
        x = self.l1_conv(x)
        x = self.l1_relu(x)
        x = self.l1_maxpool(x)

        x = self.l2_conv(x)
        x = self.l2_relu(x)
        x = self.l2_maxpool(x)

        x = self.l3_conv(x)
        x = self.l3_relu(x)
        x = self.l3_maxpool(x)

        # Fully connected layers
        x = self.fc_flat(x)
        x = self.fc_lin1(x)
        x = self.fc_relu(x)
        x = self.fc_lin2(x)

        return x

mean = [0.4914, 0.4822, 0.4465]
std = [0.2470, 0.2435, 0.2616]

#Initializing normalizing transform for the dataset
normalize_transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean = mean,
                                     std = std)])

# We will normalize our dataset

#Downloading the CIFAR10 dataset into train and test sets
train_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/train", train=True,
    transform=normalize_transform,
    download=True)



test_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/test", train=False,
    transform=normalize_transform,
    download=True)

#Generating data loaders from the corresponding datasets
batch_size = 256
train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)  # MMMR --- Shuffle needs to be true.
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)

#Selecting the appropriate training device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = CNN().to(device)

#Defining the model hyper parameters
num_epochs = 50
learning_rate = 0.001
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)

# Calculating training loss

train_loss_list = []
for epoch in range(num_epochs):
	print(f'Epoch {epoch+1}/{num_epochs}:', end = ' ')
	train_loss = 0

	#Iterating over the training dataset in batches
	model.train()
	for i, (images, labels) in enumerate(train_loader):

		#Extracting images and target labels for the batch being iterated
		images = images.to(device)
		labels = labels.to(device)

		#Calculating the model output and the cross entropy loss
		outputs = model(images)
		loss = criterion(outputs, labels)

		#Updating weights according to calculated loss
		optimizer.zero_grad()
		loss.backward()
		optimizer.step()
		train_loss+=loss.item()

	#Printing loss for each epoch
	train_loss_list.append(train_loss/len(train_loader))
	print(f"Training loss = {train_loss_list[-1]}")

#Plotting loss for all epochs
plt.plot(range(1,num_epochs+1), train_loss_list)
plt.xlabel("Number of epochs")
plt.ylabel("Training loss")
plt.show()

test_acc=0
model.eval()

with torch.no_grad():
    #Iterating over the training dataset in batches
    for i, (images, labels) in enumerate(test_loader):

        images = images.to(device)
        y_true = labels.to(device)

        #Calculating outputs for the batch being iterated
        outputs = model(images)

        #Calculated prediction labels from models
        _, y_pred = torch.max(outputs.data, 1)

        #Comparing predicted and true labels
        test_acc += (y_pred == y_true).sum().item()

    print(f"Test set accuracy = {100 * test_acc / len(test_dataset)} %")




# Implementing CNN With Resnet, this is done with Adam Optimiser
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import torchvision.datasets as dsets

mean = [0.4914, 0.4822, 0.4465]
std = [0.2470, 0.2435, 0.2616]

# Initializing normalizing transform for the dataset
normalize_transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=mean,
                                     std=std)])

# We will normalize our dataset

# Downloading the CIFAR10 dataset into train and test sets
train_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/train", train=True,
    transform=normalize_transform,
    download=True)

test_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/test", train=False,
    transform=normalize_transform,
    download=True)

# Generating data loaders from the corresponding datasets
batch_size = 256
train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,
                                           batch_size=batch_size)  # MMMR --- Shuffle needs to be true.
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)


import torch.nn as nn

class ResNetCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(ResNetCNN, self).__init__()

        # Residual blocks
        self.residual_block1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
        )

        self.residual_block2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
        )

        self.residual_block3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )

        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))

        # Fully connected layers
        self.fc1 = nn.Linear(64, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):


# Residual blocks
        x = self.residual_block1(x)
        x = self.residual_block2(x)
        x = self.residual_block3(x)

        # Global average pooling
        x = self.global_avg_pool(x)

        # Fully connected layers
        x = x.view(-1, 64)
        x = self.fc1(x)
        x = nn.ReLU()(x)
        x = self.fc2(x)

        return x


# Selecting the appropriate training device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = ResNetCNN().to(device)

# Defining the model hyper parameters
num_epochs = 50
learning_rate = 0.001
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Calculating training loss

train_loss_list = []
for epoch in range(num_epochs):
    print(f'Epoch {epoch + 1}/{num_epochs}:', end=' ')
    train_loss = 0

    # Iterating over the training dataset in batches
    model.train()
    for i, (images, labels) in enumerate(train_loader):
        # Extracting images and target labels for the batch being iterated
        images = images.to(device)
        labels = labels.to(device)

        # Calculating the model output and the cross entropy loss
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Updating weights according to calculated loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    # Printing loss for each epoch
    train_loss_list.append(train_loss / len(train_loader))
    print(f"Training loss = {train_loss_list[-1]}")

# Plotting loss for all epochs
plt.plot(range(1, num_epochs + 1), train_loss_list)
plt.xlabel("Number of epochs")
plt.ylabel("Training loss")
plt.show()

test_acc = 0
model.eval()

with torch.no_grad():
    # Iterating over the training dataset in batches
    for i, (images, labels) in enumerate(test_loader):
        images = images.to(device)
        y_true = labels.to(device)

        # Calculating outputs for the batch being iterated
        outputs = model(images)

        # Calculated prediction labels from models
        _, y_pred = torch.max(outputs.data, 1)

        # Comparing predicted and true labels
        test_acc += (y_pred == y_true).sum().item()

    print(f"Test set accuracy = {100 * test_acc / len(test_dataset)} %")

#  We have already run with normalising Resnet CNN
# Lets run without normalizing



# Implementing CNN With Resnet Without Normalisation
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import torchvision.datasets as dsets

# Downloading the CIFAR10 dataset into train and test sets
transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()])



train_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/train", train=True,
    transform=transform,
    download=True)

test_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/test", train=False,
    transform=transform,
    download=True)
# Generating data loaders from the corresponding datasets
batch_size = 256
train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,
                                           batch_size=batch_size)  # MMMR --- Shuffle needs to be true.
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)


class ResNetCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(ResNetCNN, self).__init__()

        # Residual blocks
        self.residual_block1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
        )

        self.residual_block2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
        )

        self.residual_block3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )

        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))

        # Fully connected layers
        self.fc1 = nn.Linear(64, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):


# Residual blocks
        x = self.residual_block1(x)
        x = self.residual_block2(x)
        x = self.residual_block3(x)

        # Global average pooling
        x = self.global_avg_pool(x)

        # Fully connected layers
        x = x.view(-1, 64)
        x = self.fc1(x)
        x = nn.ReLU()(x)
        x = self.fc2(x)

        return x


# Selecting the appropriate training device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = ResNetCNN().to(device)


# Defining the model hyper parameters
num_epochs = 50
learning_rate = 0.001
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Calculating training loss
print("Printing Training loss without normalisation\n")
train_acc_list = []
for epoch in range(num_epochs):
    print(f'Epoch {epoch + 1}/{num_epochs}:', end=' ')
    train_acc = 0
    count=0

    # Iterating over the training dataset in batches
    model.train()
    for i, (images, labels) in enumerate(train_loader):
        # Extracting images and target labels for the batch being iterated
        images = images.to(device)
        labels = labels.to(device)

        # Calculating the model output and the cross entropy loss
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Updating weights according to calculated loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        _, predicted = torch.max(outputs.data, 1)

        # Comparing predicted and true labels
        total = labels.size(0)
        correct = (predicted == labels).sum().item()
        train_acc += correct
        count += total
#Printing train accuracy for each epoch
    train_acc_list.append(100*train_acc / count)
    print(f"Training accuracy = {train_acc_list[-1]} %")



# Plotting loss for all epochs
plt.plot(range(1, num_epochs + 1), train_acc_list)
plt.xlabel("Number of epochs")
plt.ylabel("Training loss without normalization")
plt.show()

test_acc = 0
model.eval()

with torch.no_grad():
    # Iterating over the training dataset in batches
    for i, (images, labels) in enumerate(test_loader):
        images = images.to(device)
        y_true = labels.to(device)

        # Calculating outputs for the batch being iterated
        outputs = model(images)

        # Calculated prediction labels from models
        _, y_pred = torch.max(outputs.data, 1)

        # Comparing predicted and true labels
        test_acc += (y_pred == y_true).sum().item()

    print(f"Test set accuracy for without normalization= {100 * test_acc / len(test_dataset)} %")

#Testing different optimisers
#a) Stochastic gradient descent without momentum and mini batch of 256

# Implementing CNN With Resnet
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import torchvision.datasets as dsets

mean = [0.4914, 0.4822, 0.4465]
std = [0.2470, 0.2435, 0.2616]

# Initializing normalizing transform for the dataset
normalize_transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=mean,
                                     std=std)])

# We will normalize our dataset

# Downloading the CIFAR10 dataset into train and test sets
train_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/train", train=True,
    transform=normalize_transform,
    download=True)

test_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/test", train=False,
    transform=normalize_transform,
    download=True)

# Generating data loaders from the corresponding datasets
batch_size = 256
train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,
                                           batch_size=batch_size)  # MMMR --- Shuffle needs to be true.
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)


import torch.nn as nn

class ResNetCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(ResNetCNN, self).__init__()

        # Residual blocks
        self.residual_block1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
        )

        self.residual_block2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
        )

        self.residual_block3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )

        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))

        # Fully connected layers
        self.fc1 = nn.Linear(64, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):


# Residual blocks
        x = self.residual_block1(x)
        x = self.residual_block2(x)
        x = self.residual_block3(x)

        # Global average pooling
        x = self.global_avg_pool(x)

        # Fully connected layers
        x = x.view(-1, 64)
        x = self.fc1(x)
        x = nn.ReLU()(x)
        x = self.fc2(x)

        return x


# Selecting the appropriate training device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = ResNetCNN().to(device)

# Defining the model hyper parameters
num_epochs = 50
learning_rate = 0.001
criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)

# Calculating training loss

train_acc_list = []
for epoch in range(num_epochs):
    print(f'Epoch {epoch + 1}/{num_epochs}:', end=' ')
    train_acc = 0
    count=0

    # Iterating over the training dataset in batches
    model.train()
    for i, (images, labels) in enumerate(train_loader):
        # Extracting images and target labels for the batch being iterated
        images = images.to(device)
        labels = labels.to(device)

        # Calculating the model output and the cross entropy loss
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Updating weights according to calculated loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)

        # Comparing predicted and true labels
        total = labels.size(0)
        correct = (predicted == labels).sum().item()
        train_acc += correct
        count += total

    train_acc_list.append(100*train_acc / count)
    print(f"Training accuracy = {train_acc_list[-1]} %")

    # Printing loss for each epoch
    train_loss_list.append(train_loss / len(train_loader))


# Plotting loss for all epochs
plt.plot(range(1, num_epochs + 1), train_acc_list)
plt.xlabel("Number of epochs")
plt.ylabel("Training accuracy")
plt.show()

test_acc = 0
model.eval()

with torch.no_grad():
    # Iterating over the training dataset in batches
    for i, (images, labels) in enumerate(test_loader):
        images = images.to(device)
        y_true = labels.to(device)

        # Calculating outputs for the batch being iterated
        outputs = model(images)

        # Calculated prediction labels from models
        _, y_pred = torch.max(outputs.data, 1)

        # Comparing predicted and true labels
        test_acc += (y_pred == y_true).sum().item()

    print(f"Test set accuracy = {100 * test_acc / len(test_dataset)} %")

#b) SGD with minibatch=256 and momentum=0.9
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import torchvision.datasets as dsets

mean = [0.4914, 0.4822, 0.4465]
std = [0.2470, 0.2435, 0.2616]

# Initializing normalizing transform for the dataset
normalize_transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=mean,
                                     std=std)])

# We will normalize our dataset

# Downloading the CIFAR10 dataset into train and test sets
train_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/train", train=True,
    transform=normalize_transform,
    download=True)

test_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/test", train=False,
    transform=normalize_transform,
    download=True)

# Generating data loaders from the corresponding datasets
batch_size = 256
train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,
                                           batch_size=batch_size)  # MMMR --- Shuffle needs to be true.
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)


import torch.nn as nn

class ResNetCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(ResNetCNN, self).__init__()

        # Residual blocks
        self.residual_block1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
        )

        self.residual_block2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
        )

        self.residual_block3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )

        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))

        # Fully connected layers
        self.fc1 = nn.Linear(64, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):


# Residual blocks
        x = self.residual_block1(x)
        x = self.residual_block2(x)
        x = self.residual_block3(x)

        # Global average pooling
        x = self.global_avg_pool(x)

        # Fully connected layers
        x = x.view(-1, 64)
        x = self.fc1(x)
        x = nn.ReLU()(x)
        x = self.fc2(x)

        return x


# Selecting the appropriate training device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = ResNetCNN().to(device)

# Defining the model hyper parameters
num_epochs = 50
learning_rate = 0.001
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9)

# Calculating training loss

train_acc_list = []
for epoch in range(num_epochs):
    print(f'Epoch {epoch + 1}/{num_epochs}:', end=' ')
    train_loss = 0
    train_acc = 0
    count=0

    # Iterating over the training dataset in batches
    model.train()
    for i, (images, labels) in enumerate(train_loader):
        # Extracting images and target labels for the batch being iterated
        images = images.to(device)
        labels = labels.to(device)

        # Calculating the model output and the cross entropy loss
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Updating weights according to calculated loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)

        # Comparing predicted and true labels
        total = labels.size(0)
        correct = (predicted == labels).sum().item()
        train_acc += correct
        count += total

    train_acc_list.append(100*train_acc / count)
    print(f"Training accuracy = {train_acc_list[-1]} %")

    # Printing loss for each epoch
    train_loss_list.append(train_loss / len(train_loader))


# Plotting loss for all epochs
plt.plot(range(1, num_epochs + 1), train_acc_list)
plt.xlabel("Number of epochs")
plt.ylabel("Training loss")
plt.show()

test_acc = 0
model.eval()

with torch.no_grad():
    # Iterating over the training dataset in batches
    for i, (images, labels) in enumerate(test_loader):
        images = images.to(device)
        y_true = labels.to(device)

        # Calculating outputs for the batch being iterated
        outputs = model(images)

        # Calculated prediction labels from models
        _, y_pred = torch.max(outputs.data, 1)

        # Comparing predicted and true labels
        test_acc += (y_pred == y_true).sum().item()

    print(f"Test set accuracy = {100 * test_acc / len(test_dataset)} %")



# Test affect of Depths
#a) Implementing CNN Resnet with Four level Resnet block with two fully-connected layers
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import torchvision.datasets as dsets

mean = [0.4914, 0.4822, 0.4465]
std = [0.2470, 0.2435, 0.2616]

# Initializing normalizing transform for the dataset
normalize_transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=mean,
                                     std=std)])

# We will normalize our dataset

# Downloading the CIFAR10 dataset into train and test sets
train_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/train", train=True,
    transform=normalize_transform,
    download=True)

test_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/test", train=False,
    transform=normalize_transform,
    download=True)

# Generating data loaders from the corresponding datasets
batch_size = 256
train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,
                                           batch_size=batch_size)  # MMMR --- Shuffle needs to be true.
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)


import torch.nn as nn

class ResNetCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(ResNetCNN, self).__init__()

        # Residual blocks
        self.residual_block1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
        )

        self.residual_block2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
        )

        self.residual_block3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )

        # Add an additional residual block
        self.residual_block4 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
        )

        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))

        # Fully connected layers
        self.fc1 = nn.Linear(128, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):

        # Residual blocks
        x = self.residual_block1(x)
        x = self.residual_block2(x)
        x = self.residual_block3(x)
        x = self.residual_block4(x)

        # Global average pooling
        x = self.global_avg_pool(x)

        # Fully connected layers
        x = x.view(-1, 128)
        x = self.fc1(x)
        x = nn.ReLU()(x)
        x = self.fc2(x)

        return x



# Selecting the appropriate training device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = ResNetCNN().to(device)

# Defining the model hyper parameters
num_epochs = 50
learning_rate = 0.001
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Calculating training loss

train_loss_list = []
for epoch in range(num_epochs):
    print(f'Epoch {epoch + 1}/{num_epochs}:', end=' ')
    train_loss = 0

    # Iterating over the training dataset in batches
    model.train()
    for i, (images, labels) in enumerate(train_loader):
        # Extracting images and target labels for the batch being iterated
        images = images.to(device)
        labels = labels.to(device)

        # Calculating the model output and the cross entropy loss
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Updating weights according to calculated loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    # Printing loss for each epoch
    train_loss_list.append(train_loss / len(train_loader))
    print(f"Training loss = {train_loss_list[-1]}")

# Plotting loss for all epochs
plt.plot(range(1, num_epochs + 1), train_loss_list)
plt.xlabel("Number of epochs")
plt.ylabel("Training loss")
plt.show()

test_acc = 0
model.eval()

with torch.no_grad():
    # Iterating over the training dataset in batches
    for i, (images, labels) in enumerate(test_loader):
        images = images.to(device)
        y_true = labels.to(device)

        # Calculating outputs for the batch being iterated
        outputs = model(images)

        # Calculated prediction labels from models
        _, y_pred = torch.max(outputs.data, 1)

        # Comparing predicted and true labels
        test_acc += (y_pred == y_true).sum().item()

    print(f"Test set accuracy = {100 * test_acc / len(test_dataset)} %")

#b) Implementing CNN Resnet with Three level Resnet block with four fully-connected layers
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import torchvision.datasets as dsets

mean = [0.4914, 0.4822, 0.4465]
std = [0.2470, 0.2435, 0.2616]

# Initializing normalizing transform for the dataset
normalize_transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=mean,
                                     std=std)])

# We will normalize our dataset

# Downloading the CIFAR10 dataset into train and test sets
train_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/train", train=True,
    transform=normalize_transform,
    download=True)

test_dataset = torchvision.datasets.CIFAR10(
    root="./CIFAR10/test", train=False,
    transform=normalize_transform,
    download=True)

# Generating data loaders from the corresponding datasets
batch_size = 256
train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,
                                           batch_size=batch_size)  # MMMR --- Shuffle needs to be true.
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)


class ModifiedResNetCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(ModifiedResNetCNN, self).__init__()

        # Residual blocks (3 blocks)
        self.residual_block1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
        )

        self.residual_block2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
        )

        self.residual_block3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )

        # Global average pooling
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))

        # Fully connected layers (4 layers)
        self.fc1 = nn.Linear(64, 128)
        self.fc2 = nn.Linear(128, 256)
        self.fc3 = nn.Linear(256, 512)
        self.fc4 = nn.Linear(512, num_classes)

    def forward(self, x):
        # Residual blocks
        x = self.residual_block1(x)
        x = self.residual_block2(x)
        x = self.residual_block3(x)

        # Global average pooling
        x = self.global_avg_pool(x)

        # Fully connected layers
        x = x.view(-1, 64)
        x = self.fc1(x)
        x = nn.ReLU()(x)
        x = self.fc2(x)
        x = nn.ReLU()(x)
        x = self.fc3(x)
        x = nn.ReLU()(x)
        x = self.fc4(x)

        return x






# Selecting the appropriate training device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = ModifiedResNetCNN().to(device)

# Defining the model hyper parameters
num_epochs = 50
learning_rate = 0.001
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Calculating training loss

train_loss_list = []
for epoch in range(num_epochs):
    print(f'Epoch {epoch + 1}/{num_epochs}:', end=' ')
    train_loss = 0

    # Iterating over the training dataset in batches
    model.train()
    for i, (images, labels) in enumerate(train_loader):
        # Extracting images and target labels for the batch being iterated
        images = images.to(device)
        labels = labels.to(device)

        # Calculating the model output and the cross entropy loss
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Updating weights according to calculated loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    # Printing loss for each epoch
    train_loss_list.append(train_loss / len(train_loader))
    print(f"Training loss = {train_loss_list[-1]}")

# Plotting loss for all epochs
plt.plot(range(1, num_epochs + 1), train_loss_list)
plt.xlabel("Number of epochs")
plt.ylabel("Training loss")
plt.show()

test_acc = 0
model.eval()

with torch.no_grad():
    # Iterating over the training dataset in batches
    for i, (images, labels) in enumerate(test_loader):
        images = images.to(device)
        y_true = labels.to(device)

        # Calculating outputs for the batch being iterated
        outputs = model(images)

        # Calculated prediction labels from models
        _, y_pred = torch.max(outputs.data, 1)

        # Comparing predicted and true labels
        test_acc += (y_pred == y_true).sum().item()

    print(f"Test set accuracy = {100 * test_acc / len(test_dataset)} %")
